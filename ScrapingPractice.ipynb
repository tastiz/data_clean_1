{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapingPractice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdVtwSNKqHv8rAKVezdlO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tastiz/data_clean_1/blob/master/ScrapingPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IOhWzWGWrq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://pun.me/pages/funny-jokes.php'\n",
        "response = requests.get(url)\n",
        "if(response.ok):\n",
        "  data=response.text\n",
        "  \n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(data, 'html.parser')\n",
        "#print(soup.prettify())\n",
        "\n",
        "soup.find_all('ol')\n",
        "\n",
        "list = soup.find('ol')\n",
        "items = list.find_all('li')\n",
        "#print(items)\n",
        "\n",
        "jokes = [joke.get_text() for joke in items]\n",
        "#print(jokes)\n",
        "\n",
        "div = soup.find('div', class_='content')\n",
        "list = div.find('ol')\n",
        "items = list.find_all('li')\n",
        "jokes = [joke.get_text() for joke in items]\n",
        "#print(jokes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXuMlktaO_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc621650-45c0-48b0-b5cc-c56fd4496e9d"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.space.com/news'\n",
        "\n",
        "scrape = requests.get(url)\n",
        "if(response.ok):\n",
        "  data = scrape.text\n",
        "\n",
        "chowder = BeautifulSoup(data, 'html.parser')\n",
        "#print(chowder.prettify())\n",
        "\n",
        "story_list = chowder.find_all('div', class_='content')\n",
        "\n",
        "#print(story_list)\n",
        "\n",
        "#article names scraped and in a list\n",
        "article_name = chowder.find_all(class_='article-name')\n",
        "article =[articles.get_text() for articles in article_name]\n",
        "\n",
        "\n",
        "author_name = chowder.find_all(class_='by-author')\n",
        "author = [authors.get_text() for authors in author_name]\n",
        "print(author)\n",
        "\n",
        "published_date = chowder.find_all(class_='published-date relative date')\n",
        "\n",
        "synopsis = chowder.find_all(class_='synopsis')\n",
        "#print(synopsis)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\nBy\\n\\nElizabeth Howell \\n', '\\nBy\\n\\nRobert Z. Pearlman \\n', '\\nBy\\n\\nSpace.com Staff \\n', '\\nBy\\n\\nTariq Malik \\n', '\\nBy\\n\\nHanneke Weitering \\n', '\\nBy\\n\\nMike Wall \\n', '\\nBy\\n\\nMike Wall \\n', '\\nBy\\n\\nDoris Elin Urrutia \\n', '\\nBy\\n\\nMarshall Honorof \\n', '\\nBy\\n\\nYasemin Saplakoglu \\n', '\\nBy\\n\\nElizabeth Howell \\n', '\\nBy\\n\\nNola Taylor Redd \\n', '\\nBy\\n\\nJoe Rao \\n', '\\nBy\\n\\nElizabeth Howell \\n', '\\nBy\\n\\nMike Wall \\n', '\\nBy\\n\\nElizabeth Howell \\n', '\\nBy\\n\\nMeghan Bartels \\n', '\\nBy\\n\\nDoris Elin Urrutia \\n', '\\nBy\\n\\nMeghan Bartels \\n', '\\nBy\\n\\nMeghan Bartels \\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B76p8Dg9fpeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dcacf06-e114-4926-e0e5-27086adc9e10"
      },
      "source": [
        "url = 'https://stackoverflow.com/jobs'\n",
        "response = requests.get(url)\n",
        "\n",
        "# make sure we got a valid response\n",
        "if(response.ok):\n",
        "  # get the full data from the response\n",
        "  data = response.text\n",
        "  soup = BeautifulSoup(data, 'html.parser')\n",
        "  \n",
        "  # find all elements with class *-job-summary*\n",
        "  summary =soup.find_all(class_='-job-summary')\n",
        "  #print(summary)\n",
        "\n",
        "\n",
        "titles = soup.select('.-title')\n",
        "\n",
        "title_headings = soup.select('.-title > h2')\n",
        "print(title_headings)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}